\documentclass[11pt,letter]{article}

\usepackage[]{geometry}
\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3ex,       % <-- and this
  headsep=2ex,          % <-- and this
}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{parskip}
\usepackage{titling}
\pretitle{\begin{flushleft}\LARGE\sffamily}
\posttitle{\par\end{flushleft}\vskip 0.5em}
\preauthor{\begin{flushleft}}
\postauthor{\par\end{flushleft}}
\predate{\begin{flushleft}\scshape}
\postdate{\par\end{flushleft}}
\setlength{\droptitle}{-20pt}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\fancyhf{}
\setlength{\headheight}{5pt}
\setlength\headsep{25pt}
%\renewcommand{\headrulewidth}{0pt} % remove line at top
\lhead{\fancyplain{}{CS 4780 Final Project}}
\rhead{\fancyplain{}{jc882, yl477, jp624}}
% \rfoot{\fancyplain{}{\thepage}}
\cfoot{\thepage\ of \pageref{LastPage}}
\usepackage{lastpage}

%==Bookmarks==
\usepackage{hyperref}
\hypersetup{
    % Document Information
    pdftitle={},
    pdfauthor={},
    pdfkeywords={},
    % Link Options
    colorlinks={false},
    linkcolor={white},
    pagecolor={white},
    % Display Options
    bookmarks={true},
    bookmarksopen={true},
    pdfstartview={FitH},
    pdfpagelayout={OneColumn},
    pdfpagemode={UseOutlines}}

\begin{document}
%-------------------------------------------------------------------------------
\begin{center}
% ==================TITLE PAGE==================
% Location Information
    {\large \sc Cornell University $\bullet$ Ithaca, NY}
    \vspace{42mm}
    % Title
    \\ {\huge \textbf {CS 4780 Final Project Report\vspace{8mm}\Huge\\Predicting Criminal Sentences}}
    \vspace{22mm}
    \normalsize
    % Author
    \\ Justin Cheng
    \\ Yunchi Luo
    \\ Jane\,\,\,Jae Won\,\,\,\,Park
    % Email
    \\ \{jc882, yl477, jp624\}@cornell.edu
    
    \vspace{8mm}
    
    \vspace{12mm}
    % Date
    
    December 16, 2011
 \footnote{This version was last updated and generated on \today.}
\end{center}

\vspace{20mm}

%-------------------------------------------------------------------------------
\newpage
\tableofcontents
\newpage

%-------------------------------------------------------------------------------

%\title{CS 4780 Final Project: Predicting Crimial Sentences}
%\author{Justin Cheng \emph{jc882}, Yunchi Luo \emph{yl477}, Jane Jae Won Park \emph{jp624}}
%\date{\today}
%\maketitle

%\hrule
%\vskip 1em

\section{Introduction}

\subsection{Motivation}
If two different individuals commit an identical or similar crime that should be given same sentences by the non-discriminatory nature of our laws, is it actually the case that the two end up receiving the same sentences? The sentence is, after all, determined by a human who uses some combination of his objective knowledge of law and subjective judgment, perhaps even some whimsical impulse. 
While sociologists, behavioral economists, and researchers of law and our justice system have investigated how race, age and various factors affect what kind of sentences are handed out to criminals, they have focused on the psychology and motivations behind these results, not the objective details of the observations themselves. Existing studies on criminal justice use traditional statistical techniques and limited case studies. We seek to employ machine learning techniques to provide an objective analysis regarding which factors of a criminal or the crime are most influential in determining the felonÕs sentence.
\subsection{Statement of Task}
Can we recover the rules judges use to determine sentences by investigating information about the criminals and sentences they received? If the judges are making purely objective decisions, is it possible to reverse-engineer parts of the law by looking at the sentences that criminals received? Can we create a machine that can, with reasonable accuracy, match the decisions of the human judges and reveal the true factors that influence sentences?
\section{Methods}

\subsection{Feature Selection}

TODO: How did we select features?

\begin{enumerate}
	\item COMPLETE - Offense Completed/Inchoate
	\item COUNTY - Sentencing County
	\item DAASS - Drug and Alcohol Assessment
	\item DISP - Type of Disposition
	\item DOB - Date of Birth
	\item DOF - Date of Offense
	\item DOSAGE - Age at Sentencing
	\item DOS - Date of Sentence
	\item FINE - Amount of Fine Imposed
	\item GRADE - Statutory Offense Grade
	\item INCMIN - Minimum Length of Incarceration
	\item INCMAX - Maximum Length of Incarceration
	\item INCTYPE - Type of Incarceration
	\item PCSOFF - PCS Offense Code
	\item PCSSUB - PCS Offense Subcode
	\item RACE - Ethnicity of Offender
	\item SEX - Gender of Offender
\end{enumerate}

\subsection{Feature Processing}

TODO: What are these?

\begin{description}
	\item [\emph{b/nb} Binarization] 'Split' a multi-valued label into multiple binary variables for each label (i.e., one-hot encoding of the label)
	\item [\emph{c/nc} Coarsification] Place continuous data into a number of evenly distributed buckets, so that there are fewer values to work with.
	\item [\emph{\_/uno} Date field separation] Process date data (MM/DD/YYYY) into month, day, year, day of week, or unix time.
	\item [\emph{\_/balN}] Sorting label values into $N$ buckets, and whether the sample contained equal number of examples from each bucket
\end{description}

\subsection{Datasets}
\begin{enumerate}
	\item DEMO - SEX, DOFAGE, RACE (should add DAASS in the future)
	\item CRIME - FINE, GRADE, COMPLETE, DOS\_UNO, COUNTY, PCSOFF, PCSSUB, DOF\_UNO, DISP
	\item BASE - Features from both DEMO and CRIME
\end{enumerate}

The DEMO subset only contains demographic information, while the CRIME subset contains only details about the offense committed. The BASE subset combines features from both subsets. In all subsets, the label being predicted is INCMIN.

\subsection{Linear Regression}
We performed linear regression on all pairs of attributes, and calculated the correlation coefficient, $\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X\sigma_Y}$, for each of them. To validate the correlation coefficients obtained, we calculated the $p$-value for the null hypothesis that the correlation coefficient of two attributes is 0.

\subsection{Learning Algorithms}

\subsubsection{Decision Tree Learning}

We used an the Orange implementation of the C4.5 Decision Tree Algorithm.

\paragraph{Parameter Tuning}
Parameter tuning was carried out by trying out all possible combinations of parameters below, using the classification accuracy obtained on 10-fold cross-validation.

\begin{itemize}
	\item Parameter $m$ from $0.1$ to $100$ \\
		- Parameter used in computing M-estimate probabilities during post-pruning
	\item Maximum majority from $0.5$ to $1.0$ \\
		- Induction stops when a node has a majority at least this value
	\item Minimum examples per node from $0$ to $10$ \\
		- Minimum examples in each leaf
	\item Minimum subset from $0$ to $10$ \\
		- Minimum examples in non-null leaves
	\item Measures - information gain, information gain ratio, gini index, relief % See http://orange.biolab.si/doc//orange25/Orange.feature.scoring.html#Orange.feature.scoring.Relief
\end{itemize}

\subsubsection{kNN Learning}

\paragraph{Parameter Tuning}
\begin{itemize}
	\item $k$ from $1$ to $2\sqrt{n}$ \\
		- Since a good value of $k$ to use is generally $\sqrt{n}$
\end{itemize}

\subsection{Regression}
In addition to the binary classification task detailed above, we also used regression to find out how well we could predict features like INCMIN if they were instead continuous. We used two methods - linear regression, and SVM regression. The reason for the use of these two methods is that linear regression is fast and in many cases a sufficient model in regression classification tasks. SVM regression, while significantly slower, allows the use of kernels which could possibly improve our accuracy, which we quantify in terms of $R^2$.

We also employ the use of principle component analysis (PCA), in order to better understand which features can best predict our target variable, as well as explore the possibility of reducing the dimensionality of our data without sacrificing accuracy (i.e. principle component regression, or using the components generated by PCA in linear regression).

As in the binary task, we trained a regression classifier on 80\% of the data, while validating it on the remaining 20\%.

\section{Results and Discussion}

\subsection{Cross-Validation Set Accuracy}
The classification accuracies listed below is the accuracy obtained on 10-fold cross-validation.

\subsubsection{DEMO}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
bayes  &  0.629 & 284  &  153  &  218  &  345  \\
c4.5   &  0.659 & 353  &  192  &  149  &  306  \\
lin. svm & 0.637 & 277  &  138  &  225  &  360  \\
svm & 0.651 & ? & ? & ? & ? \\
kmeans  & 0.643 & 337  &  192  &  165  &  306  \\
\hline
\end{tabular}

\subsubsection{CRIME}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
bayes  &  0.772  & 392 &   118 &   110 &   380  \\
c4.5   &  0.783 & 404  &  119 &   98   &  379  \\
lin. svm & 0.583 & 276  &  191 &   226  &  307  \\
kmeans  & 0.769 & 381  &  110 &   121  &  388  \\
\hline
\end{tabular}

\subsubsection{BASE}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
bayes   & 0.761  &  390  &  127  &  112  &  371  \\
c4.5    & 0.774  &  396  &  120  &  106  &  378  \\
lin.svm & 0.589  &  290  &  199  &  212  &  299  \\
kmeans  & 0.773  &  400  &  125  &  102  &  373  \\
\hline
\end{tabular}

\subsection{Validation Set Accuracy}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & DEMO & CRIME & BASE \\
\hline
bayes  &  0.629 & 0.765  &  0.753   \\
c4.5   &  0.610 & 0.762  &  0.754  \\
lin.svm & 0.633 & 0.600  &  0.603   \\
svm & 0.599 & ? & ? \\
kmeans  & 0.616 & 0.777  &  0.758   \\
\hline
\end{tabular}

\subsection{Linear Regression}

\subsubsection{Correlation Heatmap}
% \includegraphics[scale=0.5]{report_figures/corr_heatmap.pdf} % Uncomment when ready to print

TODO: Uncomment above line to import heatmap

The figure above shows the correlation between all tested attributes, 
with blue indicating uncorrelated attributes and yellow indicating highly correlated attributes.

\subsubsection{Top Correlated Attributes}
Ignoring correlation between binarized attributes (ex. GRADE\_1 and GRADE\_2), 
and trivial correlations such as that between DOSAGE and DOFAGE and PCSOFF and GRADE (since unique PCSOFFs are assigned unique GRADEs in the codebook),

\begin{tabular}{|l|l|r|c|}
\hline
Attribute $X$ & Attribute $Y$ & $|\rho_{X,Y}|$ & $p$-value \\
\hline
%COUNTY\_37 (Lawrence) & DISP\_6 (Other Disposition) & 0.907 & 2.2e-16 \\
% PCSOFF\_351600 (Simple Possession) & GRADE\_1 (Unclassified Misdemeanor) & 0.786 & 2.2e-16 \\
% PCSOFF\_353000 (PWID: Drug Unknown) & GRADE\_5 (Unclassified Felony) & 0.712 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_3 (Misdemeanor 2) & 0.626 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_1 (Unclassified Misdemeanor) & 0.550 & 2.2e-16 \\
%RACE\_1 (White) & COUNTY\_51 (Philadelphia) & -0.527 & 2.2e-16 \\
%RACE\_2 (Black) & COUNTY\_51 (Philadelphia) & 0.482 & 2.2e-16 \\
%RACE\_9 (Unknown) & COUNTY\_30 (Greene) & 0.446 & 2.2e-16 \\
%INCMIN & PCSOFF\_753731 (DUI) & -0.436 & 2.2e-16 \\
%INCMIN & GRADE\_3 (Misdemeanor 2) & -0.372 & 2.2e-16 \\
%RACE\_2 (Black) & PCSOFF\_753731 (DUI) & -0.369 & 2.2e-16 \\
%COUNTY\_51 (Philadelphia) & PCSOFF\_753731 (DUI) & -0.353 & 2.2e-16 \\
COUNTY\_37&	DISP\_6&	0.69581896 & 2.2e-16\\INCMIN&	GRADE&	0.51992742 & 2.2e-16\\COUNTY\_47&	PCSOFF\_180912&	0.44711808 & 2.2e-16\\INCMIN&	PCSOFF\_753731&	0.44359416 & 2.2e-16\\COUNTY\_51&	DISP\_4&	0.40880622 & 2.2e-16\\RACE\_1&	COUNTY\_51&	0.38671562 & 2.2e-16\\INCMIN&	PCSSUB\_A&	0.34272914 & 2.2e-16\\RACE\_2&	COUNTY\_51&	0.33313766 & 2.2e-16\\RACE\_1&	PCSOFF\_753731&	0.3129321 & 2.2e-16\\COUNTY\_9&	DISP\_2&	0.31041962 & 2.2e-16\\
\hline
\end{tabular}

TODO: Analysis

\subsection{Decision Tree}
\subsubsection{Optimal Parameters}

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Dataset & $m$ & Max. majority & Min. examples & Min. subset & Measure & Accuracy \\
\hline
DEMO & 5 & 1.0 & 2 & 2 & Gini & 0.659 \\
CRIME & 100 & 1.0 & 2 & 2 & Relief & 0.783 \\
BASE & 100 & 0.8 & 1 & 2 & Relief & 0.774 \\
\hline
\end{tabular}

\paragraph{Optimal Decision Tree for DEMO} % TODO Turn this into a pretty tree?
\begin{verbatim}
RACE_1=0
|    SEX=1
|    |    DOFAGE<=39.500: 1 (69.05%)
|    |    DOFAGE>39.500
|    |    |    RACE_9=1: -1 (100.00%)
|    |    |    RACE_9=0
|    |    |    |    . . .
|    SEX=2
|    |    RACE_9=1: -1 (100.00%)
|    |    RACE_9=0
|    |    |    DOFAGE<=32.000: 1 (61.54%)
|    |    |    DOFAGE>32.000
|    |    |    |    . . .
RACE_1=1
|    DOFAGE<=24.500
|    |    DOFAGE<=19.500
|    |    |    DOFAGE<=17.500: 1 (80.00%)
|    |    |    DOFAGE>17.500: -1 (75.47%)
|    |    DOFAGE>19.500
|    |    |    SEX=1: 1 (57.94%)
|    |    |    SEX=2: -1 (83.33%)
|    DOFAGE>24.500
|    |    DOFAGE<=38.500: -1 (70.00%)
|    |    DOFAGE>38.500
|    |    |    DOFAGE>43.500: -1 (76.92%)
|    |    |    DOFAGE<=43.500
|    |    |    |    . . .
\end{verbatim}

\subsection{SVM}
\begin{tabular}{|c|c|c|c|c|}
\hline
Dataset & $C$ & $t$ & $t$-dependent. vars & Accuracy \\
\hline
DEMO & 5 & 2 (RBF) & $g=0.05$ & 0.651 \\
\hline
\end{tabular}

\subsection{kNN}
\begin{tabular}{|c|c|c|}
\hline
Dataset & $k$ & Accuracy \\
\hline
DEMO & 55 & 0.643 \\
CRIME & 28 & 0.769 \\
BASE & 42 & 0.773 \\
\hline
\end{tabular}


\subsection{Significance Tests}

\subsubsection{Binomial Sign Test}
Each cell in the table below corresponds to whether the learning algorithm in that row was significantly BETTER than the algorithm in that column, WORSE or similar ($\sim$).

\paragraph{DEMO} \quad

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & $\sim$ & $\sim$ & $\sim$ \\
SVM & - & - & $\sim$ & $\sim$ \\
kNN (Tuned) & - & - & - & $\sim$ \\
\hline
\end{tabular}

\paragraph{CRIME} \quad

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & BETTER & $\sim$ & $\sim$ \\
SVM & - & - & WORSE & WORSE \\
kNN (Tuned) & - & - & - & $\sim$ \\
\hline
\end{tabular}

\paragraph{BASE} \quad

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & BETTER & $\sim$ & $\sim$ \\
SVM & - & - & WORSE & WORSE \\
kNN (Tuned) & - & - & - & $\sim$ \\
\hline
\end{tabular}

\section{Regression Analysis}

\subsection{Linear Regression}
The results of linear regression on each dataset are summarized in Table \ref{TableLinReg}. Here, we see that the Akaike information criterion (AIC) is lowest for ALL, given that every other feature set is a subset of it. Surprisingly, ABOUT, despite having only 2 features, best predicts minimum incarceration. Nevertheless, we see here that using demographic features (DEMO) results in the highest AIC score, again demonstrating that by-and-large, sentences lengths are decided independent of one's age, gender or race.
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
  \hline
  Dataset & AIC & $R^2$ & Attr. & Coeff. & $p$-value \\
  \hline
  \multirow{2}{*}{DEMO} & \multirow{2}{*}{9.00\e{4}} & \multirow{2}{*}{1.40\e{-2}} & COUNTY48 & 0.249 & 2\e{-16} \\
  &&& COUNTY39 & 0.183 & 4.73\e{-16} \\
  \hline
  \multirow{2}{*}{CRIME} & \multirow{2}{*}{7.41\e{4}} & \multirow{2}{*}{3.55\e{-2}} & PCSOFF751543 & 2.76 & 4.73\e{-7} \\
  &&& PCSOFF353009 & 1.34 & 1.19\e{-2} \\  
  \hline
  \multirow{2}{*}{ABOUT} & \multirow{2}{*}{1.57\e{4}} & \multirow{2}{*}{3.44\e{-3}} & SEXPRED & -3.78\e{-1} & 8.55\e{-4} \\
  &&& - & - & - \\  
  \hline
  \multirow{2}{*}{HIST} & \multirow{2}{*}{2.86\e{4}} & \multirow{2}{*}{-7.82\e{-3}} & RAPC & 7.28\e{-1} & 3.49\e{-4} \\
  &&& F1C & 7.34\e{-2} & 4.43\e{-3} \\
  \hline
  \multirow{2}{*}{ALL} & \multirow{2}{*}{4.48\e{3}} & \multirow{2}{*}{1.77\e{-3}} & PCSOFF185503 & 1.08 & 3.58\e{-3} \\
  &&& COMPLETE & 1.97\e{-1} & 2.68\e{-4} \\
  \hline
  \end{tabular}
  \caption{Linear regression listing components with the highest coefficients}
  \label{TableLinReg}
\end{table}


\subsection{Principle Component Regression}
Principle component decomposition was performed on each dataset, and the top components generated, by proportion of variance in the dataset explained, are listed in Table \ref{TablePCR1}, along with the original attributes that make up each component. Table \ref{TablePCR2} shows the results of linear regression on the transformed components.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|l|}
  \hline
  Dataset & \# & V.E. & Top 5 Components \\
  \hline
  \multirow{2}{*}{DEMO} & 1 & 2.70\e{-1} & RACE1, COUNTY67, COUNTY40, COUNTY54, SEX \\
  & 2 & 7.16\e{-2} & SEX, RACE2, COUNTY2, RACE1, COUNTY23 \\
  \hline
  \multirow{2}{*}{CRIME} & 1 & 1.84\e{-1} & PCSSUB\_B, OGS, PCSSUB\_D, PCSOFF183502, PCSOFF183701 \\
  & 2 & 1.57\e{-1} & DISP2, OGS, DISP4, PCSSUB\_B, DISP5 \\
  \hline
  \multirow{2}{*}{ABOUT} & 1 & 9.67\e{-1} & DRUGDEP, SEXPRED \\
  & 2 & 3.27\e{-2} & SEXPRED, DRUGDEP \\
  \hline
  \multirow{2}{*}{HIST} & 1 & 4.13\e{-1} & PRS, MIS, F3C, DRGC, F1C \\
  & 2 & 1.29\e{-1} & MIS, AGC, M1CHILDA, WEA, M1DUIA  \\
  \hline
  \multirow{2}{*}{ALL} & 1 & 1.20\e{-1} & RACE1, PCSOFF753731, PCSSUB\_A, DISP1, DOFUNO \\
  & 2 & 8.55\e{-2} & MIS, PRS, F3C, PCSSUB\_A, DOFUNO \\
  \hline
  \end{tabular}
  \caption{PCA decomposition listing top components (\#) by variance explained (V.E.) and makeup}
  \label{TablePCR1}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Dataset & AIC & $R^2$ & \# & V.E. & Coeff. & $p$-value \\
  \hline
  \multirow{2}{*}{DEMO} & \multirow{2}{*}{9.00\e{4}} & \multirow{2}{*}{1.40\e{-2}} & 17 & 1.19\e{-2} & 2.05\e{-1} & <2\e{-16} \\
  &&& 18 & 1.14\e{-2} & -2.07\e{-1} & <2\e{-16} \\
  \hline
  \multirow{2}{*}{CRIME} & \multirow{2}{*}{7.41\e{4}} & \multirow{2}{*}{3.55\e{-2}} & 111 & 1.03\e{-4} & -1.64 & 2.58\e{-15} \\
  &&& 113 & 1.02\e{-4} & -1.89 & <2\e{-16} \\  
  \hline
  \multirow{2}{*}{ABOUT} & \multirow{2}{*}{1.57\e{4}} & \multirow{2}{*}{3.44\e{-3}} & 2 & 3.27\e{-2} & 1.19\e{-2} & 8.55\e{-4} \\
  &&& - & - & - & - \\  
  \hline
  \multirow{2}{*}{HIST} & \multirow{2}{*}{2.86\e{4}} & \multirow{2}{*}{-7.82\e{-3}} & 37 & 7.21\e{-4} & 5.97\e{-1} & 7.71\e{-3} \\
  &&& 30 & 1.14\e{-3} & -3.54\e{-1} & 4.67\e{-2} \\
  \hline
  \multirow{2}{*}{ALL} & \multirow{2}{*}{4.48\e{3}} & \multirow{2}{*}{1.77\e{-3}} & - & - & - & - \\
  &&& - & - & - & - \\
  \hline
  \end{tabular}
  \caption{PCR listing significant components with the highest absolute coefficients}
  \label{TablePCR2}
\end{table}

\subsection{SVM Regression}
SVM regression was also performed, and the results summarized in Table \ref{TableSVMReg}.
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
  \hline
  Dataset & $R^2$ & Attr. & Coeff. \\
  \hline
  \multirow{2}{*}{DEMO} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\
  \hline
  \multirow{2}{*}{CRIME} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\  
  \hline
  \multirow{2}{*}{ABOUT} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\  
  \hline
  \multirow{2}{*}{HIST} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\
  \hline
  \end{tabular}
  \caption{SVM regression listing components with the highest coefficients (in linear case)}
  \label{TableSVMReg}
\end{table}

\section{Related Work}

\section{Future Work}

\section{Conclusion}

\section{Resources}

\end{document}