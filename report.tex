\documentclass[]{article}

\usepackage[]{geometry}
\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3ex,       % <-- and this
  headsep=2ex,          % <-- and this
}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{algorithmic}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{parskip}
\usepackage{titling}
\pretitle{\begin{flushleft}\LARGE\sffamily}
\posttitle{\par\end{flushleft}\vskip 0.5em}
\preauthor{\begin{flushleft}}
\postauthor{\par\end{flushleft}}
\predate{\begin{flushleft}\scshape}
\postdate{\par\end{flushleft}}
\setlength{\droptitle}{-20pt}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\fancyhf{}
\renewcommand{\headrulewidth}{0pt} % remove line at top
\lhead{\fancyplain{}{CS 4780 Final Project}}
\rhead{\fancyplain{}{jc882, yl477, jp624}}
\rfoot{\fancyplain{}{\thepage}}

\begin{document}

\title{CS 4780 Final Project: Predicting Crimial Sentences}
\author{Justin Cheng \emph{jc882}, Yunchi Luo \emph{yl477}, Jane Jae Won Park \emph{jp624}}
\date{\today}
\maketitle

\hrule
\vskip 1em

\section{Introduction}

\subsection{Motivation}

\subsection{Statement of Task}

\section{Method}

\subsection{Feature Selection}

TODO: How did we select features?

\begin{enumerate}
	\item COMPLETE - Offense Completed/Inchoate
	\item COUNTY - Sentencing County
	\item DAASS - Drug and Alcohol Assessment
	\item DISP - Type of Disposition
	\item DOB - Date of Birth
	\item DOF - Date of Offense
	\item DOSAGE - Age at Sentencing
	\item DOS - Date of Sentence
	\item FINE - Amount of Fine Imposed
	\item GRADE - Statutory Offense Grade
	\item INCMIN - Minimum Length of Incarceration
	\item INCMAX - Maximum Length of Incarceration
	\item INCTYPE - Type of Incarceration
	\item PCSOFF - PCS Offense Code
	\item PCSSUB - PCS Offense Subcode
	\item RACE - Ethnicity of Offender
	\item SEX - Gender of Offender
\end{enumerate}

\subsection{Feature Processing}

TODO: What are these?

\begin{enumerate}
	\item \emph{b/nb} Binarization
	\item \emph{c/nc} Coarsification
	\item \emph{\_/uno} Date field separation
	\item \emph{\_/balN} Sorting label values into $N$ buckets, and whether the sample contained equal number of examples from each bucket
\end{enumerate}

\subsection{Datasets}
\begin{enumerate}
	\item DEMO - SEX, DOFAGE, RACE (should add DAASS in the future)
	\item CRIME - FINE, GRADE, COMPLETE, DOS\_UNO, COUNTY, PCSOFF, PCSSUB, DOF\_UNO, DISP
	\item BASE - Features from both DEMO and CRIME
\end{enumerate}

The DEMO subset only contains demographic information, while the CRIME subset contains only details about the offense committed. The BASE subset combines features from both subsets. In all subsets, the label being predicted is INCMIN.

\subsection{Linear Regression}
We performed linear regression on all pairs of attributes, and calculated the correlation coefficient, $\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X\sigma_Y}$, for each of them. To validate the correlation coefficients obtained, we calculated the $p$-value for the null hypothesis that the correlation coefficient of two attributes is 0.

\subsection{Learning Algorithms}

\subsubsection{Decision Tree Learning}

We used an the Orange implementation of the C4.5 Decision Tree Algorithm.

\paragraph{Parameter Tuning}
Parameter tuning was carried out by trying out all possible combinations of parameters below, using the classification accuracy obtained on 10-fold cross-validation.

\begin{itemize}
	\item Parameter $m$ from $0.1$ to $100$ \\
		- Parameter used in computing M-estimate probabilities during post-pruning
	\item Maximum majority from $0.5$ to $1.0$ \\
		- Induction stops when a node has a majority at least this value
	\item Minimum examples per node from $0$ to $10$ \\
		- Minimum examples in each leaf
	\item Minimum subset from $0$ to $10$ \\
		- Minimum examples in non-null leaves
	\item Measures - information gain, information gain ratio, gini index, relief % See http://orange.biolab.si/doc//orange25/Orange.feature.scoring.html#Orange.feature.scoring.Relief
\end{itemize}

\subsubsection{kNN Learning}

\paragraph{Parameter Tuning}
\begin{itemize}
	\item $k$ from $1$ to $2\sqrt{n}$ \\
		- Since a good value of $k$ to use is generally $\sqrt{n}$
\end{itemize}

\section{Results and Discussion}

\subsection{Overall Results}
The classification accuracies listed below is the accuracy obtained on 10-fold cross-validation.

\subsubsection{DEMO}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
bayes  &  0.629 & 284  &  153  &  218  &  345  \\
c4.5   &  0.659 & 353  &  192  &  149  &  306  \\
lin. svm & 0.637 & 277  &  138  &  225  &  360  \\
kmeans  & 0.643 & 337  &  192  &  165  &  306  \\
\hline
\end{tabular}

\subsection{Linear Regression}

\subsubsection{Correlation Heatmap}
% \includegraphics[scale=0.5]{report_figures/corr_heatmap.pdf} % Uncomment when ready to print

TODO: Uncomment above line to import heatmap

The figure above shows the correlation between all tested attributes, 
with blue indicating uncorrelated attributes and yellow indicating highly correlated attributes.

\subsubsection{Top Correlated Attributes}
Ignoring correlation between binarized attributes (ex. GRADE\_1 and GRADE\_2), 
and trivial correlations such as that between DOSAGE and DOFAGE and PCSOFF and GRADE (since unique PCSOFFs are assigned unique GRADEs in the codebook),

\begin{tabular}{|l|l|r|c|}
\hline
Attribute $X$ & Attribute $Y$ & $|\rho_{X,Y}|$ & $p$-value \\
\hline
%COUNTY\_37 (Lawrence) & DISP\_6 (Other Disposition) & 0.907 & 2.2e-16 \\
% PCSOFF\_351600 (Simple Possession) & GRADE\_1 (Unclassified Misdemeanor) & 0.786 & 2.2e-16 \\
% PCSOFF\_353000 (PWID: Drug Unknown) & GRADE\_5 (Unclassified Felony) & 0.712 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_3 (Misdemeanor 2) & 0.626 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_1 (Unclassified Misdemeanor) & 0.550 & 2.2e-16 \\
%RACE\_1 (White) & COUNTY\_51 (Philadelphia) & -0.527 & 2.2e-16 \\
%RACE\_2 (Black) & COUNTY\_51 (Philadelphia) & 0.482 & 2.2e-16 \\
%RACE\_9 (Unknown) & COUNTY\_30 (Greene) & 0.446 & 2.2e-16 \\
%INCMIN & PCSOFF\_753731 (DUI) & -0.436 & 2.2e-16 \\
%INCMIN & GRADE\_3 (Misdemeanor 2) & -0.372 & 2.2e-16 \\
%RACE\_2 (Black) & PCSOFF\_753731 (DUI) & -0.369 & 2.2e-16 \\
%COUNTY\_51 (Philadelphia) & PCSOFF\_753731 (DUI) & -0.353 & 2.2e-16 \\
COUNTY\_37&	DISP\_6&	0.69581896 & 2.2e-16\\INCMIN&	GRADE&	0.51992742 & 2.2e-16\\COUNTY\_47&	PCSOFF\_180912&	0.44711808 & 2.2e-16\\INCMIN&	PCSOFF\_753731&	0.44359416 & 2.2e-16\\COUNTY\_51&	DISP\_4&	0.40880622 & 2.2e-16\\RACE\_1&	COUNTY\_51&	0.38671562 & 2.2e-16\\INCMIN&	PCSSUB\_A&	0.34272914 & 2.2e-16\\RACE\_2&	COUNTY\_51&	0.33313766 & 2.2e-16\\RACE\_1&	PCSOFF_753731&	0.3129321 & 2.2e-16\\COUNTY\_9&	DISP\_2&	0.31041962 & 2.2e-16\\
\hline
\end{tabular}

TODO: Analysis

\subsection{Decision Tree}
\subsubsection{Optimal Parameters}

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Dataset & $m$ & Max. majority & Min. examples & Min. subset & Measure & Accuracy \\
\hline
DEMO & 5 & 1.0 & 2 & 2 & Gini & 0.659 \\
\hline
\end{tabular}

\paragraph{Optimal Decision Tree for DEMO} % TODO Turn this into a pretty tree?
\begin{verbatim}
RACE_1=0
|    SEX=1
|    |    DOFAGE<=39.500: 1 (69.05%)
|    |    DOFAGE>39.500
|    |    |    RACE_9=1: -1 (100.00%)
|    |    |    RACE_9=0
|    |    |    |    . . .
|    SEX=2
|    |    RACE_9=1: -1 (100.00%)
|    |    RACE_9=0
|    |    |    DOFAGE<=32.000: 1 (61.54%)
|    |    |    DOFAGE>32.000
|    |    |    |    . . .
RACE_1=1
|    DOFAGE<=24.500
|    |    DOFAGE<=19.500
|    |    |    DOFAGE<=17.500: 1 (80.00%)
|    |    |    DOFAGE>17.500: -1 (75.47%)
|    |    DOFAGE>19.500
|    |    |    SEX=1: 1 (57.94%)
|    |    |    SEX=2: -1 (83.33%)
|    DOFAGE>24.500
|    |    DOFAGE<=38.500: -1 (70.00%)
|    |    DOFAGE>38.500
|    |    |    DOFAGE>43.500: -1 (76.92%)
|    |    |    DOFAGE<=43.500
|    |    |    |    . . .
\end{verbatim}

\subsection{kNN}
\begin{tabular}{|c|c|c|}
\hline
Dataset & $k$ & Accuracy \\
\hline
DEMO & 55 & 0.643 \\
CRIME & 28 & 0.769 \\
\hline
\end{tabular}


\subsection{Significance Tests}

\subsubsection{Binomial Sign Test for DEMO}
Each cell in the table below corresponds to whether the learning algorithm in that row was significantly better than the algorithm in that column.

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & NO & YES & NO \\
SVM & - & - & NO & NO \\
kNN (Tuned) & - & - & - & NO \\
\hline
\end{tabular}

\paragraph{C4.5 vs kNN}
\begin{itemize}
	\item H1 CA vs H2 CA: 0.482 vs. 0.464
	\item $d1 = 23$, $d2 = 41$
	\item $p$-value: 0.0164
\end{itemize}

\paragraph{C4.5 vs Bayes}
\begin{itemize}
	\item H1 CA vs H2 CA: 0.482 vs. 0.473
	\item $d1 = 106$, $d2 = 115$
	\item $p$-value: 0.295
\end{itemize}

\paragraph{kNN vs Bayes}
\begin{itemize}
	\item H1 CA vs H2 CA: 0.464 vs. 0.473
	\item $d1 = 107$, $d2 = 98$
	\item $p$-value: 0.243
\end{itemize}

\section{Conclusion}

\section{Resources}

\end{document}