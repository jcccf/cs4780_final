\documentclass[]{article}

\usepackage[]{geometry}
\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3ex,       % <-- and this
  headsep=2ex,          % <-- and this
}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{parskip}
\usepackage{titling}
\pretitle{\begin{flushleft}\LARGE\sffamily}
\posttitle{\par\end{flushleft}\vskip 0.5em}
\preauthor{\begin{flushleft}}
\postauthor{\par\end{flushleft}}
\predate{\begin{flushleft}\scshape}
\postdate{\par\end{flushleft}}
\setlength{\droptitle}{-20pt}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\fancyhf{}
\renewcommand{\headrulewidth}{0pt} % remove line at top
\lhead{\fancyplain{}{CS 4780 Final Project}}
\rhead{\fancyplain{}{jc882, yl477, jp624}}
\rfoot{\fancyplain{}{\thepage}}

\begin{document}

\title{CS 4780 Final Project: Predicting Crimial Sentences}
\author{Justin Cheng \emph{jc882}, Yunchi Luo \emph{yl477}, Jane Jae Won Park \emph{jp624}}
\date{\today}
\maketitle

\hrule
\vskip 1em

\section{Introduction}

\subsection{Motivation}

\subsection{Statement of Task}

\section{Method}

\subsection{Feature Selection}

TODO: How did we select features?

\begin{enumerate}
	\item COMPLETE - Offense Completed/Inchoate
	\item COUNTY - Sentencing County
	\item DAASS - Drug and Alcohol Assessment
	\item DISP - Type of Disposition
	\item DOB - Date of Birth
	\item DOF - Date of Offense
	\item DOSAGE - Age at Sentencing
	\item DOS - Date of Sentence
	\item FINE - Amount of Fine Imposed
	\item GRADE - Statutory Offense Grade
	\item INCMIN - Minimum Length of Incarceration
	\item INCMAX - Maximum Length of Incarceration
	\item INCTYPE - Type of Incarceration
	\item PCSOFF - PCS Offense Code
	\item PCSSUB - PCS Offense Subcode
	\item RACE - Ethnicity of Offender
	\item SEX - Gender of Offender
\end{enumerate}

\subsection{Feature Processing}

TODO: What are these?

\begin{enumerate}
	\item \emph{b/nb} Binarization
	\item \emph{c/nc} Coarsification
	\item \emph{\_/uno} Date field separation
	\item \emph{\_/balN} Sorting label values into $N$ buckets, and whether the sample contained equal number of examples from each bucket
\end{enumerate}

\subsection{Datasets}
\begin{enumerate}
	\item DEMO - SEX, DOFAGE, RACE (should add DAASS in the future)
	\item CRIME - FINE, GRADE, COMPLETE, DOS\_UNO, COUNTY, PCSOFF, PCSSUB, DOF\_UNO, DISP
	\item BASE - Features from both DEMO and CRIME
\end{enumerate}

The DEMO subset only contains demographic information, while the CRIME subset contains only details about the offense committed. The BASE subset combines features from both subsets. In all subsets, the label being predicted is INCMIN.

\subsection{Linear Regression}
We performed linear regression on all pairs of attributes, and calculated the correlation coefficient, $\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X\sigma_Y}$, for each of them. To validate the correlation coefficients obtained, we calculated the $p$-value for the null hypothesis that the correlation coefficient of two attributes is 0.

\subsection{Learning Algorithms}

\subsubsection{Decision Tree Learning}

We used an the Orange implementation of the C4.5 Decision Tree Algorithm.

\paragraph{Parameter Tuning}
Parameter tuning was carried out by trying out all possible combinations of parameters below, using the classification accuracy obtained on 10-fold cross-validation.

\begin{itemize}
	\item Parameter $m$ from $0.1$ to $100$ \\
		- Parameter used in computing M-estimate probabilities during post-pruning
	\item Maximum majority from $0.5$ to $1.0$ \\
		- Induction stops when a node has a majority at least this value
	\item Minimum examples per node from $0$ to $10$ \\
		- Minimum examples in each leaf
	\item Minimum subset from $0$ to $10$ \\
		- Minimum examples in non-null leaves
	\item Measures - information gain, information gain ratio, gini index, relief % See http://orange.biolab.si/doc//orange25/Orange.feature.scoring.html#Orange.feature.scoring.Relief
\end{itemize}

\subsubsection{kNN Learning}

\paragraph{Parameter Tuning}
\begin{itemize}
	\item $k$ from $1$ to $2\sqrt{n}$ \\
		- Since a good value of $k$ to use is generally $\sqrt{n}$
\end{itemize}

\subsection{Regression}
In addition to the binary classification task detailed above, we also used regression to find out how well we could predict features like INCMIN if they were instead continuous. We used two methods - linear regression, and SVM regression. The reason for the use of these two methods is that linear regression is fast and in many cases a sufficient model in regression classification tasks. SVM regression, while significantly slower, allows the use of kernels which could possibly improve our accuracy, which we quantify in terms of $R^2$.

We also employ the use of principle component analysis (PCA), in order to better understand which features can best predict our target variable, as well as explore the possibility of reducing the dimensionality of our data without sacrificing accuracy (i.e. principle component regression, or using the components generated by PCA in linear regression).

As in the binary task, we trained a regression classifier on 80\% of the data, while validating it on the remaining 20\%.

\section{Results and Discussion}

\subsection{Cross-Validation Set Accuracy}
The classification accuracies listed below is the accuracy obtained on 10-fold cross-validation.

\subsubsection{DEMO}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
bayes  &  0.629 & 284  &  153  &  218  &  345  \\
c4.5   &  0.659 & 353  &  192  &  149  &  306  \\
lin. svm & 0.637 & 277  &  138  &  225  &  360  \\
svm & 0.651 & ? & ? & ? & ? \\
kmeans  & 0.643 & 337  &  192  &  165  &  306  \\
\hline
\end{tabular}

\subsubsection{CRIME}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
bayes  &  0.772  & 392 &   118 &   110 &   380  \\
c4.5   &  0.783 & 404  &  119 &   98   &  379  \\
lin. svm & 0.583 & 276  &  191 &   226  &  307  \\
kmeans  & 0.769 & 381  &  110 &   121  &  388  \\
\hline
\end{tabular}

\subsubsection{BASE}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
bayes   & 0.761  &  390  &  127  &  112  &  371  \\
c4.5    & 0.774  &  396  &  120  &  106  &  378  \\
lin.svm & 0.589  &  290  &  199  &  212  &  299  \\
kmeans  & 0.773  &  400  &  125  &  102  &  373  \\
\hline
\end{tabular}

\subsection{Validation Set Accuracy}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & DEMO & CRIME & BASE \\
\hline
bayes  &  0.629 & 0.765  &  0.753   \\
c4.5   &  0.610 & 0.762  &  0.754  \\
lin.svm & 0.633 & 0.600  &  0.603   \\
svm & 0.599 & ? & ? \\
kmeans  & 0.616 & 0.777  &  0.758   \\
\hline
\end{tabular}

\subsection{Linear Regression}

\subsubsection{Correlation Heatmap}
% \includegraphics[scale=0.5]{report_figures/corr_heatmap.pdf} % Uncomment when ready to print

TODO: Uncomment above line to import heatmap

The figure above shows the correlation between all tested attributes, 
with blue indicating uncorrelated attributes and yellow indicating highly correlated attributes.

\subsubsection{Top Correlated Attributes}
Ignoring correlation between binarized attributes (ex. GRADE\_1 and GRADE\_2), 
and trivial correlations such as that between DOSAGE and DOFAGE and PCSOFF and GRADE (since unique PCSOFFs are assigned unique GRADEs in the codebook),

\begin{tabular}{|l|l|r|c|}
\hline
Attribute $X$ & Attribute $Y$ & $|\rho_{X,Y}|$ & $p$-value \\
\hline
%COUNTY\_37 (Lawrence) & DISP\_6 (Other Disposition) & 0.907 & 2.2e-16 \\
% PCSOFF\_351600 (Simple Possession) & GRADE\_1 (Unclassified Misdemeanor) & 0.786 & 2.2e-16 \\
% PCSOFF\_353000 (PWID: Drug Unknown) & GRADE\_5 (Unclassified Felony) & 0.712 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_3 (Misdemeanor 2) & 0.626 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_1 (Unclassified Misdemeanor) & 0.550 & 2.2e-16 \\
%RACE\_1 (White) & COUNTY\_51 (Philadelphia) & -0.527 & 2.2e-16 \\
%RACE\_2 (Black) & COUNTY\_51 (Philadelphia) & 0.482 & 2.2e-16 \\
%RACE\_9 (Unknown) & COUNTY\_30 (Greene) & 0.446 & 2.2e-16 \\
%INCMIN & PCSOFF\_753731 (DUI) & -0.436 & 2.2e-16 \\
%INCMIN & GRADE\_3 (Misdemeanor 2) & -0.372 & 2.2e-16 \\
%RACE\_2 (Black) & PCSOFF\_753731 (DUI) & -0.369 & 2.2e-16 \\
%COUNTY\_51 (Philadelphia) & PCSOFF\_753731 (DUI) & -0.353 & 2.2e-16 \\
COUNTY\_37&	DISP\_6&	0.69581896 & 2.2e-16\\INCMIN&	GRADE&	0.51992742 & 2.2e-16\\COUNTY\_47&	PCSOFF\_180912&	0.44711808 & 2.2e-16\\INCMIN&	PCSOFF\_753731&	0.44359416 & 2.2e-16\\COUNTY\_51&	DISP\_4&	0.40880622 & 2.2e-16\\RACE\_1&	COUNTY\_51&	0.38671562 & 2.2e-16\\INCMIN&	PCSSUB\_A&	0.34272914 & 2.2e-16\\RACE\_2&	COUNTY\_51&	0.33313766 & 2.2e-16\\RACE\_1&	PCSOFF\_753731&	0.3129321 & 2.2e-16\\COUNTY\_9&	DISP\_2&	0.31041962 & 2.2e-16\\
\hline
\end{tabular}

TODO: Analysis

\subsection{Decision Tree}
\subsubsection{Optimal Parameters}

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Dataset & $m$ & Max. majority & Min. examples & Min. subset & Measure & Accuracy \\
\hline
DEMO & 5 & 1.0 & 2 & 2 & Gini & 0.659 \\
CRIME & 100 & 1.0 & 2 & 2 & Relief & 0.783 \\
BASE & 100 & 0.8 & 1 & 2 & Relief & 0.774 \\
\hline
\end{tabular}

\paragraph{Optimal Decision Tree for DEMO} % TODO Turn this into a pretty tree?
\begin{verbatim}
RACE_1=0
|    SEX=1
|    |    DOFAGE<=39.500: 1 (69.05%)
|    |    DOFAGE>39.500
|    |    |    RACE_9=1: -1 (100.00%)
|    |    |    RACE_9=0
|    |    |    |    . . .
|    SEX=2
|    |    RACE_9=1: -1 (100.00%)
|    |    RACE_9=0
|    |    |    DOFAGE<=32.000: 1 (61.54%)
|    |    |    DOFAGE>32.000
|    |    |    |    . . .
RACE_1=1
|    DOFAGE<=24.500
|    |    DOFAGE<=19.500
|    |    |    DOFAGE<=17.500: 1 (80.00%)
|    |    |    DOFAGE>17.500: -1 (75.47%)
|    |    DOFAGE>19.500
|    |    |    SEX=1: 1 (57.94%)
|    |    |    SEX=2: -1 (83.33%)
|    DOFAGE>24.500
|    |    DOFAGE<=38.500: -1 (70.00%)
|    |    DOFAGE>38.500
|    |    |    DOFAGE>43.500: -1 (76.92%)
|    |    |    DOFAGE<=43.500
|    |    |    |    . . .
\end{verbatim}

\subsection{SVM}
\begin{tabular}{|c|c|c|c|c|}
\hline
Dataset & $C$ & $t$ & $t$-dependent. vars & Accuracy \\
\hline
DEMO & 5 & 2 (RBF) & $g=0.05$ & 0.651 \\
\hline
\end{tabular}

\subsection{kNN}
\begin{tabular}{|c|c|c|}
\hline
Dataset & $k$ & Accuracy \\
\hline
DEMO & 55 & 0.643 \\
CRIME & 28 & 0.769 \\
BASE & 42 & 0.773 \\
\hline
\end{tabular}


\subsection{Significance Tests}

\subsubsection{Binomial Sign Test}
Each cell in the table below corresponds to whether the learning algorithm in that row was significantly BETTER than the algorithm in that column, WORSE or similar ($\sim$).

\paragraph{DEMO} \quad

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & $\sim$ & $\sim$ & $\sim$ \\
SVM & - & - & $\sim$ & $\sim$ \\
kNN (Tuned) & - & - & - & $\sim$ \\
\hline
\end{tabular}

\paragraph{CRIME} \quad

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & BETTER & $\sim$ & $\sim$ \\
SVM & - & - & WORSE & WORSE \\
kNN (Tuned) & - & - & - & $\sim$ \\
\hline
\end{tabular}

\paragraph{BASE} \quad

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & BETTER & $\sim$ & $\sim$ \\
SVM & - & - & WORSE & WORSE \\
kNN (Tuned) & - & - & - & $\sim$ \\
\hline
\end{tabular}

\section{Regression Analysis}

\subsection{Linear Regression}
The results of linear regression on each dataset are summarized in Table \ref{TableLinReg}.
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
  \hline
  Dataset & AIC & $R^2$ & Attr. & Coeff. & $p$-value \\
  \hline
  \multirow{2}{*}{DEMO} & \multirow{2}{*}{9.00\e{4}} & \multirow{2}{*}{1.40\e{-2}} & COUNTY48 & 0.249 & 2\e{-16} \\
  &&& COUNTY39 & 0.183 & 4.73\e{-16} \\
  \hline
  \multirow{2}{*}{CRIME} & \multirow{2}{*}{???\e{4}} & \multirow{2}{*}{???} & ??? & ??? & ??? \\
  &&& ??? & ??? & ??? \\  
  \hline
  \multirow{2}{*}{ABOUT} & \multirow{2}{*}{???} & \multirow{2}{*}{???} & ??? & ??? & ??? \\
  &&& ??? & ??? & ??? \\  
  \hline
  \multirow{2}{*}{HIST} & \multirow{2}{*}{???} & \multirow{2}{*}{???} & ??? & ??? & ??? \\
  &&& ??? & ??? & ??? \\
  \hline
  \end{tabular}
  \caption{Linear regression listing components with the highest coefficients}
  \label{TableLinReg}
\end{table}


\subsection{Principle Component Regression}
Principle component decomposition was performed on each dataset, and the top components generated, by proportion of variance in the dataset explained, are listed in Table \ref{TablePCR1}, along with the original attributes that make up each component. Table \ref{TablePCR2} shows the results of linear regression on the transformed components.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|l|}
  \hline
  Dataset & \# & V.E. & Top 5 Components \\
  \hline
  \multirow{2}{*}{DEMO} & 1 & 2.70\e{-1} & RACE1, COUNTY67, COUNTY40, COUNTY54, SEX \\
  & 2 & 7.16\e{-2} & SEX, RACE2, COUNTY2, RACE1, COUNTY23 \\
  \hline
  \multirow{2}{*}{CRIME} & 1 & ??? & ??? \\
  & 2 & ??? & ??? \\
  \hline
  \multirow{2}{*}{ABOUT} & 1 & ??? & ??? \\
  & 2 & ??? & ??? \\
  \hline
  \multirow{2}{*}{HIST} & 1 & ??? & ??? \\
  & 2 & ??? & ??? \\
  \hline
  \end{tabular}
  \caption{PCA decomposition listing top components (\#) by variance explained (V.E.) and makeup}
  \label{TablePCR1}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Dataset & AIC & $R^2$ & \# & V.E. & Coeff. & $p$-value \\
  \hline
  \multirow{2}{*}{DEMO} & \multirow{2}{*}{9.00\e{4}} & \multirow{2}{*}{1.40\e{-2}} & 17 & 1.19\e{-2} & 2.05\e{-1} & <2\e{-16} \\
  &&& 18 & 1.14\e{-2} & -2.07\e{-1} & <2\e{-16} \\
  \hline
  \multirow{2}{*}{CRIME} & \multirow{2}{*}{8.05\e{4}} & \multirow{2}{*}{???} & ??? & ??? & ??? & ??? \\
  &&& ??? & ??? & ??? & ??? \\  
  \hline
  \multirow{2}{*}{ABOUT} & \multirow{2}{*}{???} & \multirow{2}{*}{???} & ??? & ??? & ??? & ??? \\
  &&& ??? & ??? & ??? & ??? \\  
  \hline
  \multirow{2}{*}{HIST} & \multirow{2}{*}{???} & \multirow{2}{*}{???} & ??? & ??? & ??? & ??? \\
  &&& ??? & ??? & ??? & ??? \\
  \hline
  \end{tabular}
  \caption{PCR listing components with the highest coefficients}
  \label{TablePCR2}
\end{table}

\subsection{SVM Regression}
SVM regression was also performed, and the results summarized in Table \ref{TableSVMReg}.
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
  \hline
  Dataset & $R^2$ & Attr. & Coeff. \\
  \hline
  \multirow{2}{*}{DEMO} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\
  \hline
  \multirow{2}{*}{CRIME} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\  
  \hline
  \multirow{2}{*}{ABOUT} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\  
  \hline
  \multirow{2}{*}{HIST} & \multirow{2}{*}{???} & ??? & ??? \\
  && ??? & ??? \\
  \hline
  \end{tabular}
  \caption{SVM regression listing components with the highest coefficients (in linear case)}
  \label{TableSVMReg}
\end{table}

\section{Conclusion}

\section{Resources}

\end{document}