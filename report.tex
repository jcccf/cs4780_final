\documentclass[]{article}

\usepackage[]{geometry}
\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3ex,       % <-- and this
  headsep=2ex,          % <-- and this
}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{algorithmic}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{parskip}
\usepackage{titling}
\pretitle{\begin{flushleft}\LARGE\sffamily}
\posttitle{\par\end{flushleft}\vskip 0.5em}
\preauthor{\begin{flushleft}}
\postauthor{\par\end{flushleft}}
\predate{\begin{flushleft}\scshape}
\postdate{\par\end{flushleft}}
\setlength{\droptitle}{-20pt}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\fancyhf{}
\renewcommand{\headrulewidth}{0pt} % remove line at top
\lhead{\fancyplain{}{CS 4780 Final Project}}
\rhead{\fancyplain{}{jc882, yl477, jp624}}
\rfoot{\fancyplain{}{\thepage}}

\begin{document}

\title{CS 4780 Final Project: Predicting Crimial Sentences}
\author{Justin Cheng \emph{jc882}, Yunchi Luo \emph{yl477}, Jane Jae Won Park \emph{jp624}}
\date{\today}
\maketitle

\hrule
\vskip 1em

\section{Introduction}

\subsection{Motivation}

\subsection{Statement of Task}

\section{Method}

\subsection{Feature Selection}

TODO: How did we select features?

\begin{enumerate}
	\item COMPLETE - Offense Completed/Inchoate
	\item COUNTY - Sentencing County
	\item DAASS - Drug and Alcohol Assessment
	\item DISP - Type of Disposition
	\item DOB - Date of Birth
	\item DOF - Date of Offense
	\item DOSAGE - Age at Sentencing
	\item DOS - Date of Sentence
	\item FINE - Amount of Fine Imposed
	\item GRADE - Statutory Offense Grade
	\item INCMIN - Minimum Length of Incarceration
	\item INCMAX - Maximum Length of Incarceration
	\item INCTYPE - Type of Incarceration
	\item PCSOFF - PCS Offense Code
	\item PCSSUB - PCS Offense Subcode
	\item RACE - Ethnicity of Offender
	\item SEX - Gender of Offender
\end{enumerate}

\subsection{Feature Processing}

TODO: What are these?

\begin{enumerate}
	\item \emph{b/nb} Binarization
	\item \emph{c/nc} Coarsification
	\item \emph{\_/uno} Date field separation
	\item \emph{\_/balN} Sorting label values into $N$ buckets, and whether the sample contained equal number of examples from each bucket
\end{enumerate}

\subsection{Linear Regression}
We performed linear regression on all pairs of attributes, and calculated the correlation coefficient, $\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X\sigma_Y}$, for each of them. To validate the correlation coefficients obtained, we calculated the $p$-value for the null hypothesis that the correlation coefficient of two attributes is 0.

\subsection{Decision Tree Learning}

We used an the Orange implementation of the C4.5 Decision Tree Algorithm.

\subsubsection{Parameter Tuning}
Parameter tuning was carried out by trying out all possible combinations of parameters below, using the classification accuracy obtained on 10-fold cross-validation.

\begin{itemize}
	\item Parameter $m$ from $0.1$ to $100$ \\
		- Parameter used in computing M-estimate probabilities during post-pruning
	\item Maximum majority from $0.5$ to $1.0$ \\
		- Induction stops when a node has a majority at least this value
	\item Minimum examples per node from $0$ to $10$ \\
		- Minimum examples in each leaf
	\item Minimum subset from $0$ to $10$ \\
		- Minimum examples in non-null leaves
	\item Measures - information gain, information gain ratio, gini index, relief % See http://orange.biolab.si/doc//orange25/Orange.feature.scoring.html#Orange.feature.scoring.Relief
\end{itemize}

\section{Results and Discussion}

\subsection{Overall Results}

The classification accuracy listed below is the accuracy obtained on 10-fold cross-validation.

\begin{tabular}{|l|c|c|c|c|c|}
\hline
Algorithm & CA & TP & FP & FN & TN \\
\hline
Naive Bayes & 0.858 & 438 & 75 & 67 & 420 \\ 
C4.5 (Tuned) & 0.900 & 437 & 32 & 68 & 463 \\
Linear SVM & 0.602 & 300 & 193 & 205 & 302 \\
k-Means (Tuned) & 0.899 & 446 & 61 & 59 & 434 \\
\hline
\end{tabular}

\subsection{Linear Regression}

\subsubsection{Correlation Heatmap}
% \includegraphics[scale=0.5]{report_figures/corr_heatmap.pdf} % Uncomment when ready to print

TODO: Uncomment above line to import heatmap

The figure above shows the correlation between all tested attributes, 
with blue indicating uncorrelated attributes and yellow indicating highly correlated attributes.

\subsubsection{Top Correlated Attributes}
Ignoring correlation between binarized attributes (ex. GRADE\_1 and GRADE\_2), 
and trivial correlations such as that between DOSAGE and DOFAGE and PCSOFF and GRADE (since unique PCSOFFs are assigned unique GRADEs in the codebook),

\begin{tabular}{|l|l|r|c|}
\hline
Attribute $X$ & Attribute $Y$ & $\rho_{X,Y}$ & $p$-value \\
\hline
COUNTY\_37 (Lawrence) & DISP\_6 (Other Disposition) & 0.907 & 2.2e-16 \\
% PCSOFF\_351600 (Simple Possession) & GRADE\_1 (Unclassified Misdemeanor) & 0.786 & 2.2e-16 \\
% PCSOFF\_353000 (PWID: Drug Unknown) & GRADE\_5 (Unclassified Felony) & 0.712 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_3 (Misdemeanor 2) & 0.626 & 2.2e-16 \\
% PCSOFF\_753731 (DUI) & GRADE\_1 (Unclassified Misdemeanor) & 0.550 & 2.2e-16 \\
RACE\_1 (White) & COUNTY\_51 (Philadelphia) & -0.527 & 2.2e-16 \\
RACE\_2 (Black) & COUNTY\_51 (Philadelphia) & 0.482 & 2.2e-16 \\
RACE\_9 (Unknown) & COUNTY\_30 (Greene) & 0.446 & 2.2e-16 \\
INCMIN & PCSOFF\_753731 (DUI) & -0.436 & 2.2e-16 \\
INCMIN & GRADE\_3 (Misdemeanor 2) & -0.372 & 2.2e-16 \\
RACE\_2 (Black) & PCSOFF\_753731 (DUI) & -0.369 & 2.2e-16 \\
COUNTY\_51 (Philadelphia) & PCSOFF\_753731 (DUI) & -0.353 & 2.2e-16 \\
\hline
\end{tabular}

TODO: Analysis

\subsection{Decision Tree}
\subsubsection{Optimal Parameters on \emph{nb/nc/uno/bal2}}

% BROKEN DATA - The one with 6k examples
%\begin{itemize}
%	\item Label: INCMIN/GRADE
%	\item Attributes: DOSAGE, SEX, RACE, COUNTY, DOFAGE, PCSOFF, PCSSUB, INCTYPE, FINE, GRADE/INCMIN, DAASS, DISP, COMPLETE, DOB\_UNO, DOS\_UNO, DOF\_UNO
%\end{itemize}
%
%\begin{tabular}{|c|c|c|c|c|c|c|c|}
%\hline
%Label & INCMAX? & $m$ & Max. majority & Min. examples & Min. subset & Measure & Accuracy \\
%\hline
%INCMIN & $\checkmark$ & 2 & 1.0 & 1 & 1 & Gini & 0.963\\
%\hline
%INCMIN & - & 50 & 0.9 & 6 & 4 & Gini & 0.921 \\
%\hline
%GRADE & $\checkmark$ & 1 & 1.0 & 2 & 0 & Relief & 0.978 \\
%\hline
%GRADE & - & 0 & 1.0 & 2 & 1 & Relief & 0.979 \\
%\hline
%\end{tabular}

\begin{itemize}
	\item Label: INCMIN
	\item Attributes: DOSAGE, SEX, RACE, COUNTY, DOFAGE, INCTYPE, FINE, GRADE (2), DAASS, DISP, COMPLETE, DOB\_UNO, DOS\_UNO, DOF\_UNO
\end{itemize}

\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Label & INCMAX? & $m$ & Max. majority & Min. examples & Min. subset & Measure & Accuracy \\
\hline
INCMIN & - & 0.5 & 0.8 & 0 & 1 & Relief & 0.900 \\
\hline
\end{tabular}

\paragraph{Optimal Decision Tree} % TODO Turn this into a pretty tree?
\begin{verbatim}
INCTYPE=1: 1 (94.09%)
INCTYPE=2: -1 (88.09%)
INCTYPE=6: 1 (100.00%)
INCTYPE=3
|    DISP_2=0
|    |    RACE_2=0: -1 (92.00%)
|    |    RACE_2=1
|    |    |    FINE>25.000: -1 (100.00%)
|    |    |    FINE<=25.000
|    |    |    |    . . .
|    DISP_2=1
|    |    DOSAGE<=25.000: 1 (100.00%)
|    |    DOSAGE>25.000: -1 (85.71%)
\end{verbatim}

\subsection{kNN}
\begin{tabular}{|c|c|c|c|}
\hline
Label & INCMAX? & $k$ & Accuracy \\
\hline
INCMIN & - & 28 & 0.889 \\
\hline
\end{tabular}


\subsection{Significance Tests}

\subsubsection{Binomial Sign Test}
Each cell in the table below corresponds to whether the learning algorithm in that row was significantly better than the algorithm in that column.

\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & C4.5 (Tuned) & SVM & kNN (Tuned) & Bayes \\
\hline
C4.5 (Tuned) & - & \checkmark & \checkmark & \checkmark \\
\hline
\end{tabular}

\paragraph{C4.5 vs SVM}
\begin{itemize}
	\item H1 CA vs H2 CA: 0.912 vs. 0.497
	\item $d1 = 30$, $d2 = 445$
	\item $p$-value: $3.25e-96$
\end{itemize}

\paragraph{C4.5 vs kNN}
\begin{itemize}
	\item H1 CA vs H2 CA: 0.912 vs. 0.899
	\item $d1 = 7$, $d2 = 20$
	\item $p$-value: 0.00958
\end{itemize}

\paragraph{C4.5 vs Bayes}
\begin{itemize}
	\item H1 CA vs H2 CA: 0.912 vs. 0.866
	\item $d1 = 21$, $d2 = 67$
	\item $p$-value: 4.60e-7
\end{itemize}

\section{Conclusion}

\section{Resources}

\end{document}